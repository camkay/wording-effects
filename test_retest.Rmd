---
title: "Test-retest reliability"
date: "Last updated `r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

We also plan to evaluate test-retest reliability within formats (within session and over two weeks); we expect slightly higher test-retest reliability for item wording formats that are more specific -- formats #3 and #4 above vs the use of adjectives alone. In other words, we expect equal or lower retest reliability for the adjectives than for longer phrases. We will also consider the effect of performance on the word recall task on retest reliability .


```{r, include = FALSE}
knitr::opts_chunk$set(warning = F, message = F)
```

```{r, echo = F}
library(here) # for working with files
library(tidyverse) # for cleaning
library(janitor) # for variable names
library(lme4) # for mulitlevel modeling
library(lmerTest) # for p-values
library(emmeans) # for comparisons
library(sjPlot) # for figures
library(ggpubr) # for prettier plots
library(kableExtra) # for nicer tables
library(papaja) # for number formatting
library(psych) # for correlation tests

options(knitr.kable.NA = '')
```

```{r, echo = F}
load(here("data/items_df.Rds"))
```

## Prep dataset

The data structure needed for these analsyes is in wide-format. That is, we require one column for each time point. In addition, we hope to examine reliability _within_ format, which requires selecting only the response options which match the original, Block 1, assessment. 

```{r}
items_df = items_df %>% 
  mutate(condition = tolower(condition)) %>% 
  mutate(condition = factor(condition, 
                            levels = c("a", "b", "c", "d"),
                            labels = c("Adjective\nOnly", "Am\nAdjective", "Tend to be\nAdjective", "I am someone\nwho tends to be\nAdjective")))

items_matchb1 = items_df %>% 
  filter(format == condition) %>% 
  mutate(block = paste0("block_", block)) %>% 
  select(-timing, -seconds_log, -i) %>% 
  spread(block, response)
```

We standardize responses within each block -- this allows us to use a regression framework yet interpret the slopes as correlations. 

```{r}
items_matchb1 = items_matchb1 %>% 
  mutate(across(
    starts_with("block"), ~(.-mean(., na.rm=T))/sd(., na.rm = T)
  ))
```

We also standardize the memory scores for ease of interpretation. 

```{r}
items_matchb1 = items_matchb1 %>% 
  mutate(across(
    ends_with("memory"), ~(.-mean(., na.rm=T))/sd(., na.rm = T)
  ))
```


## Test-retest reliability (all items pooled)

To estimate the reliability coefficients, we use a multilevel model, predicting the latter block from the earlier one. These models nest responses within participant, allowing us to estimate standard errors which account for the dependency of scores.

```{r}
tr_mod1_b1b2 = lmer(block_2 ~ block_1 + (1 |proid), data = items_matchb1)
tr_mod1_b1b3 = lmer(block_3 ~ block_1 + (1 |proid), data = items_matchb1)

tab_model(tr_mod1_b1b2, tr_mod1_b1b3, show.re.var = F)
```


## Test-retest reliability (all items pooled, by format)

We fit these same models, except now we moderate by format, to determine whether the test-retest reliabilty differs as a function of item wording.

```{r, results = 'hide'}
tr_mod2_b1b2 = lmer(block_2 ~ block_1*condition + (1 |proid),
                    data = items_matchb1)
tr_mod2_b1b3 = lmer(block_3 ~ block_1*condition + (1 |proid),
                    data = items_matchb1)

tab_model(tr_mod2_b1b2, tr_mod2_b1b3, show.re.var = F)
```

We also extract the simple slopes estimates of these models, which allow us to more explicitly identify and compare the test-retest correlations.

### Block 1/Block 2

```{r}
emtrends(tr_mod2_b1b2, pairwise ~ condition, var = "block_1")
```

### Block 1/Block 3

```{r}
emtrends(tr_mod2_b1b3, pairwise ~ condition, var = "block_1")
```

## Test-retest reliability (all items pooled, by format and memory)

Here we fit models moderated by memory -- that it, perhaps the test-retest coefficient is affected by the memory of the participant.

```{r, results = 'hide'}
tr_mod3_b1b2 = lmer(block_2 ~ block_1*condition*delayed_memory + 
                      (1 |proid),
                    data = items_matchb1)
tr_mod3_b1b3 = lmer(block_3 ~ block_1*condition*very_delayed_memory + 
                      (1 |proid),
                    data = items_matchb1)

tab_model(tr_mod3_b1b2, tr_mod3_b1b3, show.re.var = F)
```

We also extract the simple slopes estimates of these models, which allow us to more explicitly identify and compare the test-retest correlations.

### Block 1/Block 2

```{r}
mem_list = list(delayed_memory = c(-1,0,1), 
                condition = unique(items_df$condition))

emtrends(tr_mod3_b1b2, 
         pairwise~condition|delayed_memory, 
         var = "block_1", 
         at = mem_list)
```

### Block 1/Block 3

**This chunk is turned off due to low coverage. Be sure to turn on with real data.**

```{r, eval = F}
mem_list = list(very_delayed_memory = c(-1,0,1), 
                condition = unique(items_df$condition))

emtrends(tr_mod3_b1b3, 
         pairwise~condition|very_delayed_memory, 
         var = "block_1", 
         at = mem_list)
```


## Test-retest reliability (items separated, by format)

To assess test-retest reliability for each item, we can rely on more simple correlation analyses, as each participant only contributed one response to each item in each block. We first not the sample size coverage for these comparisons:

```{r}
items_matchb1 %>%
  group_by(item, condition) %>%
  count() %>%
  ungroup() %>%
  full_join(expand_grid(item = unique(items_matchb1$item),
                        condition = unique(items_matchb1$condition))) %>%
  mutate(n = ifelse(is.na(n), 0, n)) %>%
  summarise(
    min = min(n),
     max = max(n),
    mean = mean(n),
    median = median(n)
  )
```

```{r testre, fig.cap = "(#fig:testre) Sample sizes for item-level test-retest correlations."}
items_matchb1 %>%
  group_by(item, condition) %>%
  count() %>%
  ungroup() %>%
  full_join(expand_grid(item = unique(items_matchb1$item),
                        condition = unique(items_matchb1$condition))) %>%
  mutate(n = ifelse(is.na(n), 0, n)) %>%
  ggplot(aes(x = n)) +
  geom_histogram(bins = 50) +
  labs(x = "Sample size",
       y = "Number of tests") +
  facet_wrap(~condition)
```


```{r, results = 'hide'}
items_cors = items_matchb1 %>%
  select(item, condition, contains("block")) %>%
  group_by(item, condition) %>%
  nest() %>%
   mutate(cors = map(data, psych::corr.test, use = "pairwise"),
         cors = map(cors, print, short = F),
         cors = map(cors, ~.x %>% mutate(comp = rownames(.)))) %>%
  select(item, condition, cors) %>%
  unnest(cols = c(cors))
```

```{r, results = 'asis'}
items_cors %>%
  mutate(raw.r = printnum(raw.r),
         raw.r = case_when(
           is.na(raw.p) ~ NA_character_,
           raw.p < .05 ~ paste0(raw.r, "*"),
           TRUE ~ raw.r)) %>%
  select(item, condition, comp, raw.r) %>%
  spread(comp, raw.r) %>%
  select(-`blc_2-blc_3`) %>% 
  kable(caption = "Test-retest correlations for each item and condition. Preregistration note: given the low sample size for the pilot data, we are missing observations for many of these comparisons. Correlations which could not be computed are blank in this table, but we expect them to be reported in the final manuscript.",
        booktabs = T) %>%
  kable_styling()
```

```{r, fig.cap = "Test-retest correlations of specific items across word format. Each dot represents the test-retest correlation within a specific item."}
items_cors %>%
  mutate(comp_num = case_when(
    comp == "blc_1-blc_2" ~ 1,
    comp == "blc_1-blc_3" ~ 2,
    comp == "blc_2-blc_3" ~ NA_real_,
  )) %>%
  filter(!is.na(comp_num)) %>% 
  ggplot(aes(x = comp_num, y = raw.r, color = condition)) +
  geom_jitter(width = .1) +
  scale_x_continuous(breaks = c(1:2),
                     labels = c("1-2", "1-3")) +
  labs(x = NULL, y = "Correlation") +
  theme_pubclean()
```

