---
title: "Format effects (Block 1 and 2 Data)"
date: "Last updated `r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

The primary aims of this study are to evaluate the effects of item wording in online, self-report personality assessment. Specifically, we intend to consider the extent to which incremental wording changes may influence differences in the distributions of responses, response times, and psychometric properties of the items. These wording changes will include a progression from using (1) trait-descriptive adjectives by themselves, (2) with the linking verb “to be” (Am…), (3) with the additional verb “to tend” (Tend to be...), and (4) with the pronoun “someone” (Am someone who tends to be…). 

Using a protocol that administers each adjective twice to the same participant (in different combinations of item format administered randomly across participants), we will use between-person analyses to compare responses using group-level data for the different formats. 

These analyses will attempt to account for memory effects by collecting data on immediate and delayed recall (5 minutes and approximately two weeks) using a memory paradigm that was developed based on a similar recall task used in the HRS (Runge et al., 2015).


```{r, include = FALSE}
knitr::opts_chunk$set(warning = F, message = F)
```

## Workspace

```{r}
library(here) # for working with files
library(tidyverse) # for cleaning
library(janitor) # for variable names
library(lme4) # for mulitlevel modeling
library(lmerTest) # for df and p-values
library(emmeans) # for pairwise comparisons
library(sjPlot) # for figures
library(ggpubr) # for prettier plots
library(kableExtra) # for nicer tables
```

```{r, echo = F}
load(here("data/cleaned_data.Rds"))
```


## Data prep

**We will use between-person analyses to compare responses using group-level data for the different formats.**

First we select the responses to the items of different formats. For this set of analyses, we use data collected in both Block 1 and Block 2 -- that is, each participant saw the same format for every item during Block 1, but a random format for each item in Block 2.

These variable names have one of two formats: `[trait]_[abcd]` (for example, `talkative_a`) or `[trait]_[abcd]_2` (for example, `talkative_a_2`). We search for these items using regular expressions.

```{r}
items_seen_b1b2 = str_subset(
  names(data),
  "^([[:alpha:]])+_[abcd](_2)?$"
)

item_responses = data %>%
  select(proid, all_of(items_seen_b1b2), memory)
```

Next we reshape these data into long form.

```{r}
item_responses = item_responses %>%
  gather(item, response, -proid, -memory) %>%
  mutate(
    block = case_when(             # which block is the item from?
      str_detect(item, "_2") ~ "2",
      TRUE ~ "1"),
    item = str_remove(item, "_2")) %>%  # remove block id from item string
  separate(item, into = c("item", "format")) %>%
  filter(!is.na(response))
```

## Response by Format

We used a multilevel model, nesting response within participant to account for dependence. Our primary predictor was format.

```{r}
item_responses$format = as.factor(item_responses$format)
item_responses$format = relevel(item_responses$format, ref = "a") 
item_responses$format = factor(item_responses$format,
                               levels = c("a","b","c","d"),
                               labels = c("Adjective\nOnly", "Am\nAdjective", "Tend to be\nAdjective", "I am someone\nwho tends to be\nAdjective"))

mod.format = lmer(response~format + (1|proid), 
                  data = item_responses)
anova(mod.format)

```

```{r, fig.cap = "Predicted response on personality items by condition."}
plot1 = plot_model(mod.format, type = "pred") 

plot1$format +
  labs(x = NULL,
       y = "Average response",
       title = "Average responses by item formatting") +
  theme_pubclean()
```

```{r, fig.cap = "Distribution of responses by category"}
means_by_group = item_responses %>%
  group_by(format) %>% 
  summarise(m = mean(response),
            s = sd(response))

item_responses %>%
  ggplot(aes(x = response)) +
  geom_histogram(aes(fill = block),
                 position = "dodge",
                 bins = 6, color = "white") +
  geom_vline(aes(xintercept = m), 
             data = means_by_group) +
  geom_text(aes(x = 1, 
                y = 200, 
                label = paste("M =", round(m,2), 
                              "\nSD =", round(s,2))), 
            data = means_by_group, 
            hjust =0, 
            vjust = 1) +
  facet_wrap(~format) +
  #guides(fill = "none") +
  scale_x_continuous(breaks = 1:6) +
  labs(y = "Number of particpants",
       title = "Distribution of responses by format") +
  theme_pubr()
```

### One model for each adjective

We can also repeat this analysis separately for each trait. 

```{r, results = 'asis'}
mod_by_item = item_responses %>%
  group_by(item) %>%
  nest() %>%
  mutate(mod = map(data, ~lmer(response~format + (1|proid), 
                               data = .))) %>%
  mutate(aov = map(mod, anova)) 

summary_by_item = mod_by_item %>%
  mutate(tidy = map(aov, broom::tidy)) %>%
  select(item, tidy) %>% 
  unnest(cols = c(tidy)) %>%
  filter(term == "format") %>%
  select(-term) %>% 
  mutate(p.adj = p.adjust(p.value, method = "holm")) 

summary_by_item %>%
  mutate(across(
    starts_with("p"),
    papaja::printnum
  )) %>%
  kable(digits = 2, booktabs = T) %>%
  kable_styling()
```

### Pairwise t-tests for significant ANOVAs

Here we identify the specific items with significant differences.

```{r}
sig_item = summary_by_item %>%
  filter(p.value < .05) 

sig_item = sig_item$item
sig_item
```

Then we create models for each adjective. We use the `emmeans` package to perform pairwise comparisons, again with a Holm correction on the _p_-values. We also plot the means and 95% confidence intervals of each mean. 

**This code will have to be changed after final data collection. It is not self-adapting!**

### Careless

```{r, results = 'asis'}
careless_model = item_responses %>%
  filter(item == "careless") %>%
  lmer(response~format + (1|proid), 
                               data = .)

careless_em = emmeans(careless_model, "format")
pairs(careless_em, adjust = "holm") %>%
  kable(booktabs = T,
        digits = c(0,2,2,2,2,3),
        col.names = c("Contrast", "Difference in means", "SE", "df", "t", "p")) %>%
  kable_styling()
```

```{r, fig.cap = "Average response to \"careless\" by format" }
plot_model(careless_model, type = "pred", terms = c("format"))
```

### Thrifty

```{r, results = 'asis'}
thrifty_model = item_responses %>%
  filter(item == "thrifty") %>%
  lmer(response~format + (1|proid), 
                               data = .)

thrifty_em = emmeans(thrifty_model, "format")
pairs(thrifty_em, adjust = "holm") %>%
  kable(booktabs = T,
        digits = c(0,2,2,2,2,3),
        col.names = c("Contrast", "Difference in means", "SE", "df", "t", "p")) %>%
  kable_styling()
```

```{r, fig.cap = "Average response to \"thrifty\" by format" }
plot_model(thrifty_model, type = "pred", terms = c("format"))
```

## Response by Format + Memory


```{r}
mod.format_mem = lmer(response~format + memory + (1|proid), 
                  data = item_responses)
anova(mod.format_mem)
summary(mod.format_mem)
```

```{r, fig.cap = "Predicted response on personality items by condition after controlling for memory."}
plot_model(mod.format_mem, type = "pred", term = c("format"))
```

```{r, fig.cap = "Predicted response on personality items by memory."}
plot_model(mod.format_mem, type = "pred", term = c("memory"))
```

### One model for each adjective

```{r, results = 'asis'}
mod_by_item = item_responses %>%
  group_by(item) %>%
  nest() %>%
  mutate(mod = map(data, ~lm(response~format + memory, data = .))) %>%
  mutate(aov = map(mod, anova)) 

summary_by_item = mod_by_item %>%
  mutate(tidy = map(aov, broom::tidy)) %>%
  select(item, tidy) %>% 
  unnest(cols = c(tidy)) %>%
  filter(term == "format") %>%
  select(-term, -df) %>% 
  mutate(p.adj = p.adjust(p.value, method = "holm"))

summary_by_item %>%
  mutate(across(
    starts_with("p"),
    papaja::printnum
  )) %>%
  kable(digits = 2, booktabs = T) %>%
  kable_styling()
```

### Pairwise t-tests for significant ANOVAs

Here we identify the specific items with significant differences.

```{r}
sig_item = summary_by_item %>%
  filter(p.value < .05) 

sig_item = sig_item$item
sig_item
```

### Careless

```{r, results = 'asis'}
careless_model = item_responses %>%
  filter(item == "careless") %>%
  lmer(response~format + memory + (1|proid), 
                               data = .)

careless_em = emmeans(careless_model, "format")
pairs(careless_em, adjust = "holm") %>%
  kable(booktabs = T,
        digits = c(0,2,2,2,2,3),
        col.names = c("Contrast", "Difference in means", "SE", "df", "t", "p")) %>%
  kable_styling()
```

```{r, fig.cap = "Average response to \"careless\" by format" }
plot_model(careless_model, type = "pred", terms = c("format"))
```


<!-- We also plan to evaluate test-retest reliability within formats (within session and over two weeks); we expect slightly higher test-retest reliability for item wording formats that are more specific -- formats #3 and #4 above vs the use of adjectives alone. -->

<!-- We will also consider the effect on retest reliability of performance on the word recall task. -->

<!-- We will further compare response times as a function of item format using both within- and between-person data in order to evaluate the presumption that adjective ratings require less time than phrased items.  -->

<!-- Finally, secondary analyses will consider preliminary evidence for differences (e.g., mean response times, mean level of response) based on the type of device used to complete the survey (mobile, desktop, or tablet). -->

## Questions

* I only used responses in Block 1 here. Should I merge the two blocks together? Or I can repeat the analyses using Block 2? One thing to consider if we merge is that I can't use the simple ANOVA for the item-level analyses (we'll have two responses per person, so back to a nested design).