---
title: "Data cleaning"
date: "Last updated `r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r 1-cleaning-1, include = FALSE}
knitr::opts_chunk$set(warning = F, message = F)
```

## Workspace

```{r 1-cleaning-2}
library(here) # for working with files
library(tidyverse) # for cleaning
library(janitor) # for variable names
library(lme4) # for multilevel modeling
library(lmerTest) # for p-values
library(sjPlot) # for figures
library(ggpubr) # for prettier plots
library(kableExtra) # for nicer tables
library(stringdist) # for scoring memory task
library(papaja) # for pretty numbers
```

## Time 1

```{r 1-cleaning-3}
data_path = here("data/Wording_July 13, 2021_20.00.text.csv")

data_labels = read_csv(data_path)

data = read_csv(data_path,
                skip = 3, 
                col_names = names(data_labels))
rm(data_labels)
data = clean_names(data)
```

Remove the following columns.

```{r 1-cleaning-4}
data = data %>%
  select(-end_date,
         -ip_address, 
         -progress, 
         -finished,
         -recorded_date,
         -external_reference, 
         -distribution_channel,
         -user_language,
         -starts_with("recipient"),
         -starts_with("location"),
         -starts_with("meta_info"))
```

### Recode personality item responses to numeric

We recode the responses to personality items, which we downloaded as text strings. 

```{r 1-cleaning-5}
p_items = str_extract(names(data), "^[[:alpha:]]*_[abcd](_2)?$")
p_items = p_items[!is.na(p_items)]

personality_items = select(data, proid, all_of(p_items))
```

Next we write a simple function to recode values.

```{r 1-cleaning-6}
recode_p = function(x){
  y = case_when(
    x == "Very inaccurate" ~ 1,
    x == "Moderately inaccurate" ~ 2,
    x == "Slightly inaccurate" ~ 3,
    x == "Slightly accurate" ~ 4,
    x == "Moderately accurate" ~ 5,
    x == "Very accurate" ~ 6,
    TRUE ~ NA_real_)
  return(y)
}
```

Finally, we apply this function to all personality items.

```{r 1-cleaning-7}
personality_items = personality_items %>%
  mutate(
    across(!c(proid), recode_p))
```

Now we merge this back into the data.

```{r 1-cleaning-8}
data = select(data, -all_of(p_items))
data = full_join(data, personality_items)
```

### Drop bots

#### Based on ID

```{r 1-cleaning-9, echo = F}
olddata = data
```

```{r rm_proid, echo = F}
data = data %>%
  mutate(proid = str_remove(proid, "Value will be set from panel or URL"),
         proid = str_remove(proid, "Value will be set from panel or UR"),
         proid = str_remove(proid, "TEST")) %>%
  filter(proid != "")

```

Prolific IDs must also be a certain length. We remove IDs that are not sufficiently long.

```{r 1-cleaning-10}
data = data %>% 
  mutate(id_length = nchar(proid)) %>% 
  filter(id_length > 20) %>% 
  select(-id_length)
```

We removed `r nrow(olddata)-nrow(data)` participants without valid Prolific IDs.

```{r 1-cleaning-11, ref.label='rm_proid'}
```

#### Based on language

```{r 1-cleaning-12, echo = F}
olddata = data
```

```{r rm_english, echo = F}
data = data %>%
  filter(english %in% c("Well", "Very well (fluent/native)"))
```

We removed `r nrow(olddata)-nrow(data)` participants that do not speak english well or very well.

#### Based on patterns

We remove any participant who provides the same response to over half of the items (17 or more items) from a given block in a row.

```{r 1-cleaning-13}
# first, identify unique adjectives, in order
adjectives = p_items %>%
  str_remove_all("_.") %>%
  unique()

# extract block 1 questions
block1 = data %>%
  select(proid, matches("^[[:alpha:]]+_[abcd]$")) 

#rename variables 
n = 0
for(i in adjectives){
  n = n+1
  names(block1) = str_replace(names(block1), i, paste0("trait", str_pad(n, 2, pad = "0")))
}


block1 = block1 %>%
  gather(item, response, -proid) %>%
  filter(!is.na(response)) %>%
  separate(item, into = c("item", "format")) %>%
  select(-format) %>%
  spread(item, response)

block1_runs = numeric(length = nrow(block1))

#   working on this!!!
for(i in 1:nrow(block1)){
  run = 0
  maxrun = 0
  for(j in 3:ncol(block1)){
    if(block1[i,j] == block1[i, j-1]){
      run = run+1
      if(run > maxrun) maxrun = run
      } else{ run = 0}
  } 
  block1_runs[i] = maxrun
}

#add to data frame
block1$block1_runs = block1_runs
```

```{r 1-cleaning-14}
# extract block 2 questions
block2 = data %>%
  select(proid, matches("^[[:alpha:]]+_[abcd]_2$")) 

#rename variables 
n = 0
for(i in adjectives){
  n = n+1
  names(block2) = str_replace(names(block2), i, paste0("trait", str_pad(n, 2, pad = "0")))
}

block2 = block2 %>%
  gather(item, response, -proid) %>%
  filter(!is.na(response)) %>%
  mutate(item = str_remove(item, "_2")) %>%
  separate(item, into = c("item", "format")) %>%
  select(-format) %>%
  spread(item, response)

block2_runs = numeric(length = nrow(block2))

#   working on this!!!
for(i in 1:nrow(block2)){
  run = 0
  maxrun = 0
  for(j in 3:ncol(block2)){
    if(block2[i,j] == block2[i, j-1]){
      run = run+1
      if(run > maxrun) maxrun = run
      } else{ run = 0}
  } 
  block2_runs[i] = maxrun
}

#add to data frame
block2$block2_runs = block2_runs
```

```{r 1-cleaning-15}
#combine results
runs_data = block1 %>%
  select(proid, block1_runs) %>%
  full_join(select(block2, proid, block2_runs)) %>%
  mutate(
    remove = case_when(
      block1_runs >= 17 ~ "Remove",
      block2_runs >= 17 ~ "Remove",
      TRUE ~ "Keep"
    ))
```

```{r 1-cleaning-16, fig.cap = "Maximum number of same consecutive responses in personality blocks."}
#visualize
runs_data %>%
  ggplot(aes(block1_runs, block2_runs)) +
  geom_point(aes(color = remove)) +
  scale_color_manual(values = c("black", "red")) +
  guides(color = "none") +
  labs(
    x = "block 1 runs",
    y = "block 2 runs"
  ) +
  theme_pubr()
```

There were `r table(runs_data$remove)[[2]]` participants who provided the same answer 17 or more times in a row. These participants were removed from the analyses.

```{r 1-cleaning-17}
data = data %>%
  full_join(select(runs_data, proid, remove)) %>%
  filter(remove != "Remove") %>%
  select(-remove)

rm(runs_data)
```

#### Based on inattentive responding

We expect to exclude any participant who has an average response of 4 (“slightly agree”) or greater to the attention check items. Two items from the Inattentive and Deviant Responding Inventory for Adjectives (IDRIA) scale (Kay & Saucier, in prep) have been included here, in part to help evaluate the extent of inattentive responding but also to consider the effect of item wording on these items. The two items used here (i.e., “Asleep”, “Human”) were chosen to be as inconspicuous as possible, so as to not to inflate item response durations. The frequency item (i.e., “human”) will be reverse-scored, so that higher scores on both the infrequency and frequency items reflect greater inattentive responding.

```{r 1-cleaning-18}
in_average = data %>%
  # reverse score human
  mutate(across(matches("^human"),  ~(.x*-1)+7)) %>%
  # select id and attention check items
  select(proid, matches("^human"), matches("^asleep")) %>% 
  gather(item, response, -proid) %>%
  filter(!is.na(response)) %>%
  group_by(proid) %>%
  summarise(avg = mean(response)) %>%
  mutate(
    remove = case_when(
      avg >= 4 ~ "Remove",
      TRUE ~ "Keep"))
```
```{r 1-cleaning-19, fig.cap = "Average response to inattention check items"}
in_average %>%
  ggplot(aes(x = avg, fill = remove)) +
  geom_histogram(bins = 20, color = "white") +
  geom_vline(aes(xintercept = 4)) +
  guides(fill = "none") +
  labs(x = "Average response to inattention check items") +
  theme_pubr()
```

We remove `r table(in_average$remove[[2]])` participants whose responses suggest inattention.

```{r 1-cleaning-20}
data = data %>%
  full_join(select(in_average, proid, remove)) %>%
  filter(remove != "Remove") %>%
  select(-remove)
```

#### Based on average time to respond to personality items

First, select just the timing of the personality items. We do this by searching for specific strings: "t_[someword]_[a or b or c or d]_(maybe 2_)_page_submit."

```{r 1-cleaning-21}
timing_data = data %>%
  select(proid, matches("t_[[:alpha:]]*_[abcd](_2)?_page_submit"))
```

Next we gather into long form and remove missing timing values

```{r 1-cleaning-22}
timing_data = timing_data %>%
  gather(variable, timing, -proid) %>%
  filter(!is.na(timing))

timing_data
```

To check, each participant should have the same number of responses: 62.

```{r 1-cleaning-23}
timing_data %>%
  group_by(proid) %>%
  count() %>%
  ungroup() %>% 
  summarise(min(n), max(n))
```

Excellent! Now we calculate the average response time per item for each participant. We mark a participant for removal if their average time is less than 1 second or greater than 30. See Figure \@ref(fig:timing_dist) for a distribution of average response time.

```{r 1-cleaning-24}
timing_data = timing_data %>%
  group_by(proid) %>%
  summarise(m_time = mean(timing)) %>%
  mutate(remove = case_when(
    m_time < 1 ~ "Remove",
    m_time > 30 ~ "Remove",
    TRUE ~ "Keep"
  ))
```

```{r timing_dist, fig.cap = "Distribution of average time to respond to personality items."}

timing_data %>%
  ggplot(aes(x = m_time, fill = remove)) +
  geom_histogram(color = "white") +
  labs(x = "Average response time (seconds)", y = "Number of participants") +
  theme_pubr()

```

```{r 1-cleaning-25, echo = F}
olddata = data
```

```{r 1-cleaning-26}
data = inner_join(data, filter(timing_data, remove == "Keep")) %>%
  select(-remove)
```

Based on timing, we removed `r printnum(nrow(olddata)-nrow(data))` participants.

We create a variable which indicates the Block 1 condition of each participant. This is used in two places: first, in recruitinig participants at Time 2 (participants are given the same format at Time 2 as they received in Block 1), and second, in selecting the corret items during the test-retest analyses. 

```{r 1-cleaning-27}
data = data %>% 
  mutate(condition = case_when(
    !is.na(outgoing_a) ~ "A",
    !is.na(outgoing_b) ~ "B",
    !is.na(outgoing_c) ~ "C",
    !is.na(outgoing_d) ~ "D",
  ))
```

At this point, we'll extract the Prolific ID numbers. These participants will be eligible to take the survey at Time 2.

```{r 1-cleaning-28}
data %>% 
  select(proid, condition) %>% 
  write_csv(file = here("data/elligible_proid"))
```

## Time 2

```{r 1-cleaning-29}
data_path_2A = here("data/Wording 2A_July 29, 2021_14.49.text.csv")

data_labels_2A = read_csv(data_path_2A)

data_2A = read_csv(data_path_2A,
                skip = 3, 
                col_names = names(data_labels_2A))
rm(data_labels_2A)
data_2A = clean_names(data_2A)
```

```{r 1-cleaning-30}
data_path_2B = here("data/Wording 2B_July 29, 2021_14.52.text (1).csv")

data_labels_2B = read_csv(data_path_2B)

data_2B = read_csv(data_path_2B,
                skip = 3, 
                col_names = names(data_labels_2B))
rm(data_labels_2B)
data_2B = clean_names(data_2B)
```

```{r 1-cleaning-31}
names(data_2B) = str_replace(names(data_2B), "caring_b_3_1", "caring_b_3i")
```


```{r 1-cleaning-32}
data_path_2C = here("data/Wording 2C_August 3, 2021_18.02.csv")

data_labels_2C = read_csv(data_path_2C)

data_2C = read_csv(data_path_2C,
                skip = 3, 
                col_names = names(data_labels_2C))
rm(data_labels_2C)
data_2C = clean_names(data_2C)
```


```{r 1-cleaning-33}
data_path_2D = here("data/Wording 2D_July 29, 2021_14.55.text.csv")

data_labels_2D = read_csv(data_path_2D)

data_2D = read_csv(data_path_2D,
                skip = 3, 
                col_names = names(data_labels_2D))
rm(data_labels_2D)
data_2D = clean_names(data_2D)
```

```{r 1-cleaning-34}
data_2 = data_2A %>% 
  full_join(data_2B) %>% 
  full_join(data_2C) %>% 
  full_join(data_2D)
```

Remove the following columns.

```{r 1-cleaning-35}
data_2 = data_2 %>%
  select(-end_date,
         -ip_address, 
         -progress, 
         -finished,
         -recorded_date,
         -external_reference, 
         -distribution_channel,
         -user_language,
         -starts_with("recipient"),
         -starts_with("location"),
         -starts_with("meta_info"))
```

### Recode personality item responses to numeric

We recode the responses to personality items, which we downloaded as text strings. Here, all items end with `_3` and sometimes with `i`. 

```{r 1-cleaning-36}
p_items_2 = str_extract(names(data_2), "^[[:alpha:]]*_[abcd]_3(i)?$")
p_items_2 = p_items_2[!is.na(p_items_2)]

personality_items_2 = select(data_2, proid, all_of(p_items_2))
```

We apply the recoding function to all personality items.

```{r 1-cleaning-37}
personality_items_2 = personality_items_2 %>%
  mutate(
    across(!c(proid), recode_p))
```

Now we merge this back into the data_2.

```{r 1-cleaning-38}
data_2 = select(data_2, -all_of(p_items_2))
data_2 = full_join(data_2, personality_items_2)
```

### Drop bots

#### Based on ID

```{r 1-cleaning-39, echo = F}
olddata_2 = data_2
```

```{r rm_proid_2, echo = F}
data_2 = data_2 %>%
  mutate(proid = str_remove(proid, "Value will be set from panel or URL"),
         proid = str_remove(proid, "Value will be set from panel or UR"),
         proid = str_remove(proid, "TEST")) %>%
  filter(proid != "")
```

We also check that the ID in time 2 matches an ID in time 1.

```{r 1-cleaning-40}
data_2 = data_2 %>% 
  filter(proid %in% data$proid)
```


We removed `r nrow(olddata_2)-nrow(data_2)` participants without valid Prolific IDs.

```{r 1-cleaning-41, ref.label='rm_proid_2'}
```

```{r 1-cleaning-42, echo = F}
olddata_2 = data_2
```

#### Based on patterns

We remove any participant who provides the same response to over half of the items (17 or more items) from a given block in a row.

```{r 1-cleaning-43}
# first, identify unique adjectives, in order
adjectives = p_items_2 %>%
  str_remove_all("_.") %>%
  unique()

# extract block 3 questions
block3 = data_2 %>%
  select(proid, all_of(p_items_2)) 

#rename variables 
n = 0
for(i in adjectives){
  n = n+1
  names(block3) = str_replace(names(block3), i, paste0("trait", str_pad(n, 2, pad = "0")))
}


block3 = block3 %>%
  gather(item, response, -proid) %>%
  filter(!is.na(response)) %>%
  mutate(item = str_remove(item, "_3(i)?$")) %>% 
  separate(item, into = c("item", "format")) %>%
  #select(-format) %>%
  spread(item, response)

block3_runs = numeric(length = nrow(block3))

for(i in 1:nrow(block3)){
  run = 0
  maxrun = 0
  for(j in 3:ncol(block3)){
    if(block3[i,j] == block3[i, j-1]){
      run = run+1
      if(run > maxrun) maxrun = run
      } else{ run = 0}
  } 
  block3_runs[i] = maxrun
}

#add to data_2 frame
block3$block3_runs = block3_runs
```


```{r 1-cleaning-44}
#combine results
runs_data_2 = block3 %>%
  select(proid, block3_runs) %>%
  mutate(
    remove = case_when(
      block3_runs >= 17 ~ "Remove",
      TRUE ~ "Keep"
    ))
```

```{r 1-cleaning-45, fig.cap = "Maximum number of same consecutive responses in personality block 3."}
#visualize
runs_data_2 %>%
  ggplot(aes(block3_runs)) +
  geom_histogram(aes(fill = remove), bins = 10,color = "white") +
  scale_color_manual() +
  guides(fill = "none") +
  labs(x = "Maximum number of repeated answers",
       y = "Participant count") +
  theme_pubr()
```

```{r, echo = F}
n_removed = table(runs_data_2$remove)
n_removed = ifelse(length(n_removed) == 1, 0, n_removed[[2]])
```


There were `r n_removed` participants who provided the same answer 17 or more times in a row. These participants were removed from the analyses.

```{r 1-cleaning-46}
data_2 = data_2 %>%
  full_join(select(runs_data_2, proid, remove)) %>%
  filter(remove != "Remove") %>%
  select(-remove)

rm(runs_data_2)
```

#### Based on inattentive responding

We expect to exclude any participant who has an average response of 4 (“slightly agree”) or greater to the attention check items. Two items from the Inattentive and Deviant Responding Inventory for Adjectives (IDRIA) scale (Kay & Saucier, in prep) have been included here, in part to help evaluate the extent of inattentive responding but also to consider the effect of item wording on these items. The two items used here (i.e., “Asleep”, “Human”) were chosen to be as inconspicuous as possible, so as to not to inflate item response durations. The frequency item (i.e., “human”) will be reverse-scored, so that higher scores on both the infrequency and frequency items reflect greater inattentive responding.

```{r 1-cleaning-47}
in_average = data_2 %>%
  # reverse score human
  mutate(across(matches("^human"),  ~(.x*-1)+7)) %>%
  # select id and attention check items
  select(proid, matches("^human"), matches("^asleep")) %>% 
  gather(item, response, -proid) %>%
  filter(!is.na(response)) %>%
  group_by(proid) %>%
  summarise(avg = mean(response)) %>%
  mutate(
    remove = case_when(
      avg >= 4 ~ "Remove",
      TRUE ~ "Keep"))
```
```{r 1-cleaning-48, fig.cap = "Average response to inattention check items"}
in_average %>%
  ggplot(aes(x = avg, fill = remove)) +
  geom_histogram(bins = 20, color = "white") +
  geom_vline(aes(xintercept = 4)) +
  guides(fill = "none") +
  labs(x = "Average response to inattention check items") +
  theme_pubr()
```

We remove `r table(in_average$remove[[2]])` participants whose responses suggest inattention.

```{r 1-cleaning-49}
data_2 = data_2 %>%
  full_join(select(in_average, proid, remove)) %>%
  filter(remove != "Remove") %>%
  select(-remove)
```

#### Based on average time to respond to personality items

First, select just the timing of the personality items. We do this by searching for specific strings: "t_[someword]_[a or b or c or d]_(maybe 2_)_page_submit."

```{r 1-cleaning-50}
timing_data_2 = data_2 %>%
  select(proid, matches("t_[[:alpha:]]*_[abcd]_3(i)?_page_submit"))
```

Next we gather into long form and remove missing timing values

```{r 1-cleaning-51}
timing_data_2 = timing_data_2 %>%
  gather(variable, timing, -proid) %>%
  filter(!is.na(timing))

timing_data_2
```

To check, each participant should have the same number of responses: 62.

```{r 1-cleaning-52}
timing_data_2 %>%
  group_by(proid) %>%
  count() %>%
  ungroup() %>% 
  summarise(min(n), max(n))
```

Excellent! Now we calculate the average response time per item for each participant. We mark a participant for removal if their average time is less than 1 second or greater than 30. See Figure \@ref(fig:timing_dist) for a distribution of average response time.

```{r 1-cleaning-53}
timing_data_2 = timing_data_2 %>%
  group_by(proid) %>%
  summarise(m_time = mean(timing)) %>%
  mutate(remove = case_when(
    m_time < 1 ~ "Remove",
    m_time > 30 ~ "Remove",
    TRUE ~ "Keep"
  ))
```

```{r timing_dist_2, fig.cap = "Distribution of average time to respond to personality items in Block 3."}

timing_data_2 %>%
  ggplot(aes(x = m_time, fill = remove)) +
  geom_histogram(color = "white") +
  labs(x = "Average response time (seconds)", y = "Number of participants") +
  theme_pubr()

```

```{r 1-cleaning-54, echo = F}
olddata_2 = data_2
```

```{r 1-cleaning-55}
data_2 = inner_join(data_2, filter(timing_data_2, remove == "Keep")) %>%
  select(-remove)
```

### Merge all datasets together

```{r 1-cleaning-56}
data_2 = data_2 %>% 
  select(proid, contains("_3"))

data = data %>% full_join(data_2)
```


## All data
### Reverse score personality items

The following items are (typically) negatively correlated with the others: reckless, moody, worrying, nervous, careless, impulsive. We reverse-score them to ease interpretation of associations and means in the later sections. In short, all traits will be scored such that larger numbers are indicative of the more socially desirable end of the specturm.

```{r 1-cleaning-57}
data = data %>%
  mutate(
    across(matches("^reckless"),  ~(.x*-1)+7),
    across(matches("^moody"),     ~(.x*-1)+7),
    across(matches("^worrying"),  ~(.x*-1)+7),
    across(matches("^nervous"),   ~(.x*-1)+7),
    across(matches("^careless"),  ~(.x*-1)+7),
    across(matches("^impulsive"), ~(.x*-1)+7))
  
```


### Score memory task

Now we score the memory task. We start by creating vectors of the correct responses. 

```{r 1-cleaning-58}
correct1 = c("book", "child", "gold", "hotel", "king", 
             "market", "paper", "river", "skin", "tree")

correct2 = c("butter", "college", "dollar", "earth", "flag", 
             "home", "machine", "ocean", "sky", "wife")

correct3 = c("blood", "corner", "engine", "girl", "house", 
             "letter", "rock", "shoes", "valley", "woman")

correct4 = c("baby", "church", "doctor", "fire", "garden", 
             "palace", "sea", "table", "village", "water")
```

Next we convert all responses to lowercase. Then we break the string of responses into a vector containing many strings.

```{r 1-cleaning-59}
data = data %>%
  mutate(
    across(matches("recall"),tolower), # convert to lower
    #replace carriage return with space
    across(matches("recall"), str_replace_all, pattern = "\\n", replacement = ","),
    # remove spaces
    across(matches("recall"), str_replace_all, pattern = " ", replacement = ","),
    # remove doubles
    across(matches("recall"), str_replace_all, pattern = ",,", replacement = ","),
    #remove last comma
    across(matches("recall"), str_remove, pattern = ",$"),
    # split the strings based on the spaces
    across(matches("recall"), str_split, pattern = ","))
```

#### Immediate recall

Now we use the `amatch` function in the `stringdist` package to look for exact (or close) matches to the target words. This function returns for each word either the position of the key in which you can find the target word or `NA` to indicate the word or a close match does not exist in the string.

```{r 1-cleaning-60}
distance = 1 #maximum distance between target word and correct response
data = data %>%
  mutate(
    memory1 = map(recall1, ~sapply(., amatch, correct1, maxDist = distance)),
    memory2 = map(recall2, ~sapply(., amatch, correct2, maxDist = distance)),
    memory3 = map(recall3, ~sapply(., amatch, correct3, maxDist = distance)),
    memory4 = map(recall4, ~sapply(., amatch, correct4, maxDist = distance))
    )
```

We count the number of correct answers. This gets complicated...

```{r 1-cleaning-61}
data = data %>%
  mutate(
    across(starts_with("memory"),
      #replace position with 1
      ~map(., sapply, FUN = function(x) ifelse(x >0, 1, 0))),
    across(starts_with("recall"),
           # are there non-missing values in the original response?
           ~map_dbl(., 
                    .f = function(x) sum(!is.na(x))), 
           .names = "{.col}_miss"),
    across(starts_with("memory"),
      #replace position with 1
      # count the number of correct answers
      ~map_dbl(., sum, na.rm=T))) %>%
  mutate(
    memory1 = case_when(
      # if there were no resposes, make the answer NA
      recall1_miss == 0 ~ NA_real_,
      # otherwise, the number of correct guesses
      TRUE ~ memory1),
    memory2 = case_when(
      recall2_miss == 0 ~ NA_real_,
      TRUE ~ memory2),
    memory3 = case_when(
      recall3_miss == 0 ~ NA_real_,
      TRUE ~ memory3),
    memory4 = case_when(
      recall4_miss == 0 ~ NA_real_,
      TRUE ~ memory4)) %>%
  # no longer need the missing count variables
  select(-ends_with("miss"))
```

Finally, we want to go from 4 columns (one for each recall test), to two: one that has the number of correct resposnes, and one that indicates which version they saw.

```{r 1-cleaning-62, results = 'asis'}
data = data %>%
  select(proid, starts_with("memory")) %>%
  gather(mem_condition, memory, -proid) %>%
  filter(!is.na(memory)) %>%
  mutate(mem_condition = str_remove(mem_condition, "memory")) %>%
  full_join(data)
```

Participants remember on average `r papaja::printnum(mean(data$memory))` words correctly $(SD = `r papaja::printnum(sd(data$memory))`)$,

```{r 1-cleaning-63, fig.cap = "Correct responses on the memory task"}
data %>%
  ggplot(aes(x = memory)) +
  geom_histogram(bins = 11, color = "white") +
  labs(x = "Number of correct responses") +
  scale_x_continuous(breaks = 0:10) +
  theme_pubr()
```

```{r 1-cleaning-64}
data %>%
  group_by(mem_condition) %>%
  summarise(
    m = mean(memory),
    s = sd(memory),
    min = min(memory),
    max = max(memory),
    n = n()
  ) %>%
  kable(booktabs = T,
        col.names = c("Condition", "Mean", "SD", "Min", "Max", "N"),
        digits = c(0, 2, 2, 1, 1, 1),
        caption = "Memory responses by condition") %>%
  kable_styling()
```

#### Delayed recall

A challenge with the delayed recall task is identifying the memory condition that participants were assigned to, but this is made easier by the work done above.

```{r 1-cleaning-65}
mem2 = data %>%
  select(proid, mem_condition, delayed_recall) %>%
  mutate(newid = 1:nrow(.))

mem2 = mem2 %>%
  mutate(
    delayed_recall1 = map(delayed_recall, ~sapply(., amatch, correct1, maxDist = distance)),
    delayed_recall2 = map(delayed_recall, ~sapply(., amatch, correct2, maxDist = distance)),
    delayed_recall3 = map(delayed_recall, ~sapply(., amatch, correct3, maxDist = distance)),
    delayed_recall4 = map(delayed_recall, ~sapply(., amatch, correct4, maxDist = distance))
    ) %>%
  gather(variable, delayed_memory, delayed_recall1:delayed_recall4)

mem2 = mem2 %>%
  mutate(
      delayed_memory = map(delayed_memory, sapply, 
                  FUN = function(x) ifelse(x >0, 1, 0)),
      # count the number of correct answers
      delayed_memory = map_dbl(delayed_memory, sum, na.rm=T))

mem2 = mem2 %>%
  group_by(proid) %>%
  filter(delayed_memory == max(delayed_memory)) %>%
  filter(row_number() == 1 ) %>% 
  select(-delayed_recall, -variable)

data = inner_join(data, mem2)
```

```{r 1-cleaning-66, fig.cap = "Distribution of delayed memory scores"}
data %>%
  ggplot(aes(x = delayed_memory)) +
  geom_histogram(color = "white", bins = 11) +
  scale_x_continuous("Number correct", breaks = c(0:10)) +
  labs(y = "Number of participants") +
  theme_pubr() 
```


```{r 1-cleaning-67, fig.cap = "Relationship between immediate and delayed recall"}
data %>%
  ggplot(aes(x = memory, y = delayed_memory)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous("Immediate number correct", breaks = c(0:10)) +
  scale_y_continuous("Delayed number correct", breaks = c(0:10)) +
  labs(title = paste0("r = ", printnum(cor(data$memory, data$delayed_memory, use = "pairwise")))) +
  theme_pubr() 
```

### Change labels of device variable

These labels are too long!

```{r 1-cleaning-68}
data = data %>% 
  mutate(devicetype = factor(
    devicetype,
    levels = c("Desktop or laptop computer", "Mobile", 
               "Tablet (for example, iPad, Galaxy Tablet, Amazon Fire, etc.)"), #what is this?!
    labels = c("Computer", "Mobile", "Tablet")
  ))
```

### Long-form dataset

```{r 1-cleaning-69, echo = F}
#oops
names(data) = str_replace(names(data), "responsbile", "responsible")
```


We need one dataset that contains the responses to and timing of the personality items in long form. This will be used for nearly all the statistical models, which will nest items within person. To create this, we first select the responses to the items of different formats. For this set of analyses, we use data collected in both Block 1 and Block 2 -- that is, each participant saw the same format for every item during Block 1, but a random format for each item in Block 2.

These variable names have one of four formats: `[trait]_[abcd]` (for example, `talkative_a`), `[trait]_[abcd]_2` (for example, `talkative_a_2`), `[trait]_[abcd]_3` (e.g., `talkative_a_3`), or `[trait]_[abcd]_3i` (e.g., `talkative_a_3i`). We search for these items using regular expressions.

```{r 1-cleaning-70}
item_responses = str_subset(
  names(data),
  "^([[:alpha:]])+_[abcd](_2)?(_3)?(i)?$"
)
```

Similarly, we'll need to know how long it took participants to respond to these items. These variable names have one of four formats listed above followed by the string `page_submit`. We search for these items using regular expressions.

```{r 1-cleaning-71}
item_timing = str_subset(names(data), "t_([[:alpha:]])+_[abcd](_2)?(_3)?(i)?_page_submit$")
```

We extract just the participant IDs, delayed memory, and these variables. 

```{r 1-cleaning-72}
items_df = data %>% 
  select(proid, condition, memory, delayed_memory, devicetype, 
         all_of(item_responses), all_of(item_timing))
```

Next we reshape these data into long form. This requires several steps. We'll need to identify whether each value is a response or timing; we can use the presence of the string `t_` for this. Next, we'll identify the block based on whether the string contains `_2` or `_3`. We also identify whether it ends with `i`, indicating the item in block 3 started with "I". Then, we identify the condition based on which letter (`a`, `b`, `c`, or `d`) follows an underscore. Throughout, we'll strip the item string of extraneous information until we're left with only the adjective assessed. Finally, we'll use spread to create separate columns for the response and the timing variables.

```{r 1-cleaning-73}
items_df = items_df %>%
  gather(item, value, all_of(item_responses), all_of(item_timing)) %>%
  filter(!is.na(value)) %>% 
  # identify whether timing or response
  mutate(variable = ifelse(str_detect(item, "^t_"), "timing", "response"),
         item = str_remove(item, "^t_"),
         item = str_remove(item, "_page_submit$")) %>% 
  #identify block
  mutate(
    block = case_when(
      str_detect(item, "_2") ~ "2", 
      str_detect(item, "_3") ~ "3", 
      TRUE ~ "1"),
    item = str_remove(item, "_[23]")) %>% 
  # identify presence of "I"
  mutate(i = case_when(
    str_detect(item, "i$") ~ "Present",
    TRUE ~ "Absent"),
    item = str_remove(item, "i$")) %>% 
  separate(item, into = c("item", "format")) %>%
  spread(variable, value)
```

We also remove responses to the adjectives "human" and "asleep", as these are not personality items per-se and included for the purpose of attention checks.

```{r 1-cleaning-74}
items_df = items_df %>% 
  filter(item != "human") %>% 
  filter(item != "asleep")
```


We give labels to the formats, to clarify interpretations and aid table and figure construction.

```{r 1-cleaning-75}
items_df$format = as.factor(items_df$format)
items_df$format = relevel(items_df$format, ref = "a") 
items_df$format = factor(items_df$format,
                               levels = c("a","b","c","d"),
                               labels = c("Adjective\nOnly", "Am\nAdjective", "Tend to be\nAdjective", "I am someone\nwho tends to be\nAdjective"))
```

#### Transform seconds

The variable `seconds` appears to have a very severe right skew. We log-transform this variable for later analyses.

```{r 1-cleaning-76}
items_df = items_df %>% 
  mutate(seconds_log = log(timing))
```

```{r 1-cleaning-77, fig.cap= "Distribution of seconds, raw and transformed."}
items_df %>% 
  gather(variable, value, timing, seconds_log) %>% 
  mutate(variable = factor(variable,
                           levels = c("timing", "seconds_log"),
                           labels = c("Seconds (raw)", "Seconds (log)"))) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 100) +
  facet_wrap(~variable, scales = "free") +
  labs(x = NULL, y = "Number of participants") +
  theme_pubr()
```



```{r 1-cleaning-78, echo = F}
save(data, file = here("data/cleaned_data.Rds"))
save(items_df, file = here("data/items_df.Rds"))
```

